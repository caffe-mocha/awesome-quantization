# Awesome Quantization Paper lists with Codes
Awesome Quantization Paper lists with Codes

 

Quantization 분류

Quantized weights + activation, during training and test (Training aware Quantization)

- LSQ
- QIL
- LQ-Nets
- ACIQ
- QIL

Quantized weights + activation, during test (Post training Quantization)

- OCS

Quantized weights + activation, during test, without data

- ZeroQ



Quantize method 이름 / 논문 제목 / conference / paper / github / official, unofficial / pytorch, tensorflow

```
|Quantize method|Title|conference|code|
|------|---|---|---|
|Quantized weights + activation, during training and test (Training aware Quantization)|
|LSQ|[Learned Step Size Quantization](https://arxiv.org/abs/1902.08153)|ICLR 2020|[Official, Tensorflow](https://github.com/microsoft/LQ-Nets)|
|테스트1|테스트2|테스트3|테스트3|
|테스트1|테스트2|테스트3|테스트3|
```



Awesome

[Awesome Deep Neural Network Compression github](https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression)



Deep Compression

paper

[Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, Song Han et al, ICLR2016(oral)](https://arxiv.org/abs/1510.00149)

code

[nn-compression github, (Pytorch, unofficial)](https://github.com/synxlin/nn-compression)







LQ-Nets

paper

[Learned Quantization for Highly Accurate and Compact Deep Neural Networks. D.Zhang et al, ECCV 2018](https://arxiv.org/pdf/1807.10029.pdf)

code

[LQ-Nets github (Official, Tensorflow)](https://github.com/microsoft/LQ-Nets)





BNN

Paper

[Binarized Neural Networks, Itay Hubara et al, NIPS 2016](https://papers.nips.cc/paper/6573-binarized-neural-networks)

code

[BNN.pytorch (Official, Pytorch)](https://github.com/itayhubara/BinaryNet.pytorch)





XNOR-Net

paper

[XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks, Mohammad Rastegari et al](https://arxiv.org/abs/1603.05279)

code

[XNOR-Net github (Unofficial, pytorch)](https://github.com/jiecaoyu/XNOR-Net-PyTorch)



LSQ

paper

[Learned Step Size Quantization, Steven K. Esser et al, ICLR 2020](https://arxiv.org/abs/1902.08153)

code

[LSQuantization github (Unofficial, pytorch)](https://github.com/hustzxd/LSQuantization)



OCS

paper

[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting, Ritchie Zhao et al, ICML 2019](https://arxiv.org/abs/1901.09504)

code

[Outlier Channel Splitting github (Official, Pytorch)](https://github.com/cornell-zhang/dnn-quant-ocs)





ACIQ

paper

[ACIQ: Analytical Clipping for Integer Quantization of neural networks, Ron Banner et al](https://openreview.net/forum?id=B1x33sC9KQ)

code

[ACIQ github (Unofficial, pytorch)](https://github.com/submission2019/cnn-quantization)



QIL

paper

[Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss, Sangil Jung et el](Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss)

code

[Re-implemented Codes for Network Compression github (Unofficial, Pytorch)](https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression/tree/master/Codes)



ZeroQ

paper

[ZeroQ: A Novel Zero Shot Quantization Framework, Yaohui Cai et al, CVPR 2020](https://arxiv.org/abs/2001.00281)

code

[ZeroQ github, (Pytorch)](https://github.com/amirgholami/ZeroQ)